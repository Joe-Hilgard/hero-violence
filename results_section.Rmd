---
title: "Hero Violence"
author: "Joe Hilgard"
date: "June 7, 2017"
output: 
  word_document:
    reference_docx: template_0.docx
---

```{r, echo = F, warning=F, message=F}
library(devtools)
#install_github("Joe-Hilgard/hilgard")
library(car)
library(pscl)
library(hilgard)
library(tidyverse)
library(broom)
library(cowplot)
load("./R-objects/analysis.RData")
```

## Sample
An initial data collection included 216 subjects, 191 of which were successfully intercepted by the confederate as they left the laboratory. During data collection, the Spring semester ended before data collection was completed.  We looked at the data to examine whether it was trending in the hypothesized direction, with the idea that we would continue data collection as planned if it was, and would drop data collection if it was not.  The data was indeed trending in the hypothesized direction, so we picked up data collection as originally planned in the following Fall semester.  This natural pause in data collection and evaluation of whether or not to continue collecting data may or may not influence Type I error rates. Additionally, we decided to exclude a semester's worth of participants from one study site (*n* = 31, 21 of which were intercepted), as the confederate used for only that semester did not correctly collect data as was instructed. To support this exclusion of the data, we found the participants solicited by this confederate were statistically less likely to volunteer than other participants, `r report_kt(badgrouptest)`.  This left us with a final sample of 204 subjects, 189 of which were successfully intercepted by the confederate. This left us with a final sample of 204 subjects, 189 of which were successfully intercepted. 

Our primary analyses concern this final sample. However, to explore the robustness of our results, we also analyze the data including those 31 excluded participants.

## Distribution

Initial inspection of the data revealed pronounced non-normality (Figure 1). Many subjects did not offer to make any calls, and among thosewho did, the number of calls offered was strongly right-skewed and favored multiples of 5. 

```{r, echo = F, warning=F, message=F}
dat.4 %>% 
  mutate(Request = factor(Request, 
                          levels=c(1,2), 
                          labels=c("Help Red Cross", "Save Lives"))) %>% 
  ggplot(aes(x = Calls)) +
  geom_histogram() +
  facet_grid(Game ~ Request) +
  theme_bw()
```

This non-normality creates ambiguity in what might be the most appropriate model to fit. Although ANOVA is commonly used and fairly robust to outliers, its assumptions would seem to be violated by the pronounced non-normality of the residuals (see e.g. Glass, Peckham, and Sanders, 1972). 
Therefore, we fit alternative models to explore the robustness of the result and to characterize the analytic ambiguity of the results. In total, we fit:
1) ANOVA
2) logistic regression, testing whether condition affected the odds of volunteering to make any calls
3) a chi-squared test, again testing whether condition affected the odds of volunteering to make any calls
4) a zero-inflated negative binomial, which accounts for the frequent zero-responses and the strong right skew, and
5) a non-parametric Kruskal-Wallis test for differences in the median.
Results for each model are reported separately, then synthesized and summarized at the end.

### ANOVA

ANOVA indicated a significant effect of game, `r report_f(aovmod[[4]], "Game")`. Neither the effect of request nor the Game × Request interaction was statistically significant, `r report_f(aovmod[[4]], "Request")` and `r report_f(aovmod[[4]], "Game:Request")`, respectively.  

Collapsing across levels of request, a contrast between the prosocial-violence and antisocial-violence conditions was statistically significant, `r report_t(aovmod.c[[4]], "GameProsocial")`. Neither game significantly differed from control. 

### Logistic GLM
To test whether the game influenced the odds of volunteering, we collapsed observations to a binomial outcome (0 = did not volunteer, 1 = volunteered). A logistic GLM was fit to test for effects.

```{r, message = F, echo = F, warning=F, include = F}
Anova(logist[[4]], type = 3)
summary(logist[[4]])
```
The effect of game was not statistically significant, but not especially far from significance either, `r report_LRchisq(logist[[4]], "Game")`. Effects of request and the Game x Request interaction were not statistically significant, `r report_LRchisq(logist[[4]], "Request")` and `r report_LRchisq(logist[[4]], "Game:Request")`, respectively.

```{r, echo = F, warning=F, message = F, include = F}
summary(logist.c[[4]])
```

Again, the contrast between the prosocial-violent and antisocial-violent games was statistically significant, `r report_z(logist.c[[4]], "GameProsocial")`. However, neither game significantly differed from control.

## Chi-squared test
```{r, echo = F, warning=F, message = F, include = F}
chisq[[4]]
chisq.c[[4]]
```
A chi-squared test was used to test whether the probability of volunteering at all differed across game conditions  (collapsing across levels of request). The omnibus test for game type was not significant, but not far from significance, `r report_kt(chisq[[4]])`. As in other analyses, the contrast between prosocial-violent and antisocial-violent was just significant, `r report_kt(chisq.c[[4]])`, although neither game significantly differed from control.

## Zero-inflated negative binomial

A zero-inflated negative binomial was then fit to the data. This model has two parts: The zero-inflation model, which estimates whether participants volunteer at all, and the count model, which estimates how many calls they offer when they do volunteer.

```{r, echo = F, warning=F, message=F, include=F}
Anova(zinbmod[[4]], type = 3)
summary(zinbmod[[4]])

```

Within the zero-inflated negative binomial model, the effect of game was ambiguous, neither statistically significant nor particularly far from significance (*chisq*(2) = 5.09, *p* = .079). Effects of request (*chisq*(1) = 0.85, *p* = .358) or the Game × Request interaction (*chisq*(2) = 0.11, *p* = .948) were not significant. The overdispersion parameter was highly significant (*z* = 4.66, *p* < .001), supporting the use of the negative binomial over a zero-inflated Poisson distribution.

```{r, echo = F, warning=F, message = F, include = F}
summary(zinbmod.c[[4]])
```

A pairwise contrast between the prosocial-violent and antisocial-violent games did not yield a significant omnibus test for an effect of game, *chisq*(2) = 3.35, *p* = .187. Tests of individual parameters, however, found a significant difference on the zero-inflation parameter (*z* = -2.14, *p* = .032) but not on the count parameter (*z* = 1.46, *p* = .146). Again, neither game differed from control.

## Kruskal-Wallis test
The Kruskal-Wallis test is a nonparametric test of group medians. Because the test is only applicable to one-way ANOVA designs and the primary hypothesis regarded effects of game, we collapsed across levels of request and tested for an effect of game. 

```{r, echo = F, warning=F, message=F, include=F}
(kruskal[[4]])
```

The omnibus test of different group means was only marginally significant, *chisq*(2) = 5.09, p = .079. 

```{r, echo = F, warning=F, message=F, include=F}
(kruskal.c[[4]])
```

The pairwise contrast between the antisocial and prosocial violent games was statistically significant, *chisq*(1) = 4.97, *p* = .026. However, neither game significantly differed from control, *p* = .224, .262.

# Describing uncertainty
## Uncertainty across models
Figure 2A shows the distribution of omnibus test *p*-values across the five models. Figure 2B shows the distribution of pairwise prosocial/antisocial game contrast *p*-values across the five models. 

```{r, echo = F, message=F, warning=F}
omnibus.p <- filter(omnibus.results, dataset == 4, term == "Game")
contrast.p <- filter(contrast.results, dataset == 4, 
                     term %in% c("Game", "GameProsocial")) 


  
f2a <- omnibus.p %>% 
  arrange(p.value) %>% 
  mutate(., row = 1:nrow(.),
         sig = p.value < .05) %>% 
  ggplot(aes(x = row, y = p.value, shape = sig)) +
  scale_shape_identity(aes(solid = sig)) +
  geom_point(size = 3) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 1)) +
  ggtitle("\tOmnibus test results") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  guides(col = F)
f2b <- contrast.p %>% 
  arrange(p.value) %>% 
  mutate(., row = 1:nrow(.),
         sig = p.value < .05) %>% 
  ggplot(aes(x = row, y = p.value, shape = sig)) +
  scale_shape_identity(aes(solid = sig)) +
  geom_point(size = 3) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 1)) +
  ggtitle("\tContrast test results") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  guides(col = F) 
plot_grid(f2a, f2b, labels = c("A", "B"), ncol = 2)
```

Across models, the omnibus test is generally not statistically significant (median *p* = `r round(median(omnibus.p$p.value), 3)`), reaching a minimum p-value of .043 in the ANOVA model. The pairwise contrast, on the other hand, is generally statistically significant. Its *p*-values range from .015 (ANOVA) to .146 (zero-inflated negative binomial model's zero-inflation parameter), with median *p* = `r round(median(contrast.p$p.value), 3)`. 

## Uncertainty across datasets
In the analyses presented above, 31 participants were excluded over concerns regarding quality of the research assistants' application of the methods.
[GIBSON, LUEKE, AM I SAYING THIS RIGHT?]
One may wonder how robust the results are to the inclusion of these subjects. Figure 3 shows the variability in the *p*-value across this decision to include or exclude. Six parameters in two datasets yields twelve *p*-values. Figure 3A shows the distribution of omnibus test *p*-values, and Figure 3B shows the distribution of pairwise contrast *p*-values across the five models. 

```{r, echo = F, message=F, warning=F}
f3a <- filter(omnibus.results, 
              term == "Game", 
              dataset %in% c(1, 4)) %>% 
  arrange(p.value) %>% 
  mutate(., row = 1:nrow(.),
         sig = p.value < .05) %>% 
  ggplot(aes(x = row, y = p.value, col = sig)) +
  geom_point() +
  scale_y_continuous(limits = c(0, 1)) +
  ggtitle("\tOmnibus test results") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  guides(col = F)
f3b <- filter(contrast.results, 
              term %in% c("Game", "GameProsocial"),
              dataset %in% c(1,4)) %>% 
  arrange(p.value) %>% 
  mutate(., row = 1:nrow(.),
         sig = p.value < .05) %>% 
  ggplot(aes(x = row, y = p.value, col = sig)) +
  geom_point() +
  scale_y_continuous(limits = c(0, 1)) +
  ggtitle("\tContrast test results") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  guides(col = F)
plot_grid(f3a, f3b, labels = c("A", "B"), ncol = 2)
```

Together, these figures illustrate the uncertainty in the *p*-value across datasets. Again, we note that only one analysis yielded a significant omnibus test. Additionally, the pairwise contrast is no longer statistically significant when including the 31 subjects of questionable quality. 
Naturally, more data is better, and bad data should be excluded. Still, to the extent that this decision was made *ad hoc*, it is useful to note their influence on the *p*-value.

# Summary
We generally did not detect a significant overall effect of game on prosocial behavior. Still, pairwise contrasts suggested a significant difference in behavior following the prosocial-violent as compared to the antisocial-violent game. However, this contrast should be interpreted with caution given that neither violent game significantly differed from the control game.

Considerable uncertainty remains. The type I error rate will be influenced by our decision to collect additional data after the initial collection. The decision to exclude subjects is justifiable, but not based on any *a priori* rule, creating another researcher degree of freedom. The prosocial-violence vs. antisocial-violence contrast is one of three potential contrasts that consider the effects of game. Finally, the results are a little sensitive to the particular model applied. Whereas a typical manuscript might conceal the ambiguity by presenting just one analysis, here we find it most helpful to show all possible analyses.

This uncertainty should not be mistaken for an argument that games do not influence prosocial behavior. Although statistical significance is sporadic and sensitive to choice of model, contrast, or inclusion rules, it is generally not the case that the results constitute clear support for the null hypothesis.

The results of this study are nonetheless helpful in estimating the effects of antisocial-violent and prosocial-violent games. Although it is unclear whether these games have effects that differ from that of a control game, we did learn that if such effects exist, they are not large and obvious. Furthermore, we are able to rule out some effects of opposite direction. The data are inconsistent with antisocial-violent games *promoting* prosocial behavior or prosocial-violent games *inhibiting* prosocial behavior.

In conclusion, despite collection of a moderately-sized sample, the results are ambiguous regarding whether prosocial-violent and antisocial-violent games affect prosocial behavior as compared to a nonviolent game. Some support for a difference between antisocial and prosocial violent games was found, but this difference was sensitive to the choice of model and dataset. Future research is encouraged to test the effects of prosocial, antisocial, violent, and nonviolent games on prosocial behavior. This research program would be facilitated by the development of more sensitive and model-friendly measures of prosocial behavior.