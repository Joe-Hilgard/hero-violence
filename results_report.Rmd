---
title: "Hero Violence"
author: "Joe Hilgard"
date: "June 7, 2017"
output: html_document
---

## Data accounting

The first copy of the data I have was archived on January 10th, 2015. It has 216 observations in total and 192 observations for "Calls". The 24 missing observations for "Calls" are each labeled as not having been intercepted. One subject is marked as zero calls but not intercepted.

The second copy of the data I have was received Feb 19, 2016 and archived Feb 24. It has 204 observations in total and 190 observations for "Calls". The 14 missing observations for "Calls" are each labeled as not having been intercepted. One subject is marked as zero calls but not intercepted.

I am a little nervous to see that the total N has decreased despite our collecting more data. When I asked about this before, I think I was told that some rows were removed in data cleaning as other rows were added in the new data collection. It is generally preferable not to delete rows from the master dataset, but rather to flag them for removal and to have an original master dataset and a cleaned dataset for analysis. 

I would like it if we could account for the excluded observations, whatever the reason for their exclusion. I request the 21-word disclaimer on everything I review, which includes "We report [...] all data exclusions." It would be a little hypocritical of me to submit a paper without the same transparency about how many observations were excluded and why.

It would also be helpful if we knew how many subjects were added after our first peek at the data.

Okay, on to the main event.

## Distribution

The data are clearly not normally distributed. Many subjects do not offer to make calls, and among those that do, the number of calls offered is 1) strongly right-skewed and 2) shows spikes at multiples of 5, as you might expect.

```{r, echo = F, warning=F, message=F}
source("HeroViolence.R")
# library(haven)
# library(tidyverse)
# dat.new <- read_sav("HeroViolenceData - FS 14 dataAdam.sav")
ggplot(dat, aes(x = Calls, fill = factor(Game))) +
  geom_histogram(position = "dodge", binwidth = 1)
```

With this in mind, ANOVA is probably not quite appropriate, because our residuals are highly non-normal.

```{r, , echo = F, warning=F}
lm(Calls ~ Game * Request, data = dat) %>% 
  augment() %>% 
  ggplot(aes(x = .resid)) + geom_histogram()
```

Ideally, we would use a model that can better account for 1) the difference between making no calls and some calls and 2) the big right skew in the number of calls. 
Treating the outcome as binary (yes calls vs. no calls) accounts for point 1 and is easy to implement as a chi-square test or a logistic regression.
Using a Zero-Inflated Poisson or Negative Binomial distribution accounts for both points 1 and 2. These models check for group differences in responding 0 vs any other number, and they fit right-skewed count data.

Alternatively, we could try a non-parametric test like the Kruskal-Wallis test that tests whether the medians of all groups are equal.

### ANOVA

Suppose we ignore the issues and just go ahead and run an ANOVA. For the effect of game, I get F(2, 183) = 3.20, p = .043. Neither game is significantly different from control, but taken together, the variance of the mean across the three groups is just statistically significant. 

### Logistic GLM
We can use GLM to see if the game played is associated with the tendency to volunteer for any number of calls (0, 1).

```{r, , echo = F, warning=F}
binom <- glm(DV ~ Game * Request, contrasts = list(Request = contr.sum), data = dat)
Anova(binom, type = 3)
summary(binom)
```
Here we get a marginally significant effect of Game

```{r, , echo = F, warning=F}
binom.contrast <- glm(DV ~ Game * Request, contrasts = list(Request = contr.sum), data = dat, subset = Game %in% c("Prosocial", "Antisocial"))
summary(binom.contrast)
```

We can get a just-significant antisocial-vs-prosocial contrast, however, t(122) = 2.23, p = .028.

## Zero-inflated negative binomial

Let's fit the more appropriate zero-inflated negative binomial. This model has two parts: The zero-inflation model, which estimates whether participants volunteer at all, and the count model, which estimates how many calls they offer when they do volunteer.

```{r, , echo = F, warning=F}
summary(zinbmod)
```

When we do this, we don't see any significant effects of the games or request on either the count or zero-inflation model. We do see a highly significant Log(theta) parameter, which means that the count data are overdispered, justifying the decision to use the negative binomial.

We can try specifying a planned contrast, by which we expect antisocial-violent to be the least helpful (-1), prosocial-violent to be the most helpful (+1), and control to be somewhere in between. But this doesn't reveal a significant effect either.

```{r, , echo = F, warning=F}
Anova(zinbmod.ordered)
```

Again, it's possible to look at just the pairwise antisocial-vs-prosocial contrast, but this doesn't reveal an effect either.

```{r, , echo = F, warning=F}
zinbmod.contrast <- zeroinfl(Calls ~ Game * Request, data = dat, dist = "negbin",
                             subset = Game %in% c("Antisocial", "Prosocial"))
summary(zinbmod.contrast)
```


## Kruskal-Wallis test
The Kruskal-Wallis test of different group means is only marginally significant, chi-sq(2) = 5.09, p = .079. If we compare just the Gratuitous Violence and Hero-Violence groups, then we get a significant result, chi-sq(1) = 4.97, p = .026, but neither of the other pairwise tests are significant (p = .224, .262).

# Summary
The results generally seem to be a little ambiguous. We can just reach statistical significance if we ignore the problems with the distribution. I don't recommend this, because it could be embarrassing in peer review, pre- or post-publication. Use of the zero-inflated negative binomial indicates an absence of significant effects. However, use of logistic regression suggests slight evidence that the prosocial game lead to greater likelihood of making any calls relative to the antisocial game, and the non-parametric KW test indicates a possible difference in means between the two conditions. 

Still, the absence of a significant omnibus test, combined with the p-values all being .05 > p > .025 and the decision to continue analysis after a peek at the data means that we may face criticism if we try to make a firm argment for the alternative hypothesis. Perhaps it would be possible to report the experiment as having ambiguous results and publish it in an open-access journal. I am fond of Royal Society Open Science, which does not charge any publication fee. I am, of course, open to publication in a more mainstream journal if the lead authors want to pursue it.

Please let me know if you have a preference for which analyses to highlight in the manuscript. 
It might be interesting to showcase the ambiguity of interpretation as a function of the marginal p-values and choice of model.
